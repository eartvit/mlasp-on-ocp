{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7449aba8-9b91-4265-8406-b34bca561a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5698b57-2887-4b51-a8e7-8c7aa8514ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b99c9-801d-471d-a895-0925874c85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34794a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac816ae9-57fe-443b-a298-411395db32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import timedelta, datetime\n",
    "local_tz = pytz.timezone('America/Toronto') # Set local timezone for InfluxDB based times calculations\n",
    "today=datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7839bf0-de93-4191-bb7b-04f426be3a6b",
   "metadata": {},
   "source": [
    "#### Note: if you did not create new data for the model training, a sample is provided in this repository. \n",
    "#### To use it, ensure that in the next cell the instruction with the file name lt_results_2022-10-01.csv in it is executed and not the one using \"today's\" date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44351c78-7bd2-4ffc-8029-f971e62d6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('lt_results_'+today+'.csv', index_col='DateTime', parse_dates=True, infer_datetime_format=True)\n",
    "data = pd.read_csv('lt_results_2022-10-01.csv', index_col='DateTime', parse_dates=True, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3eaae-82c8-4c5b-976f-705aedf80ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435b5e3-39c5-4066-b449-1854f7479d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['req2xx', 'testDurationSeconds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451236eb-36ce-4da6-b805-7d43345c50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data.mean_tps,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0) # we are setting the seed here\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db82e83-775c-4ed9-bb92-33eb69eba066",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = 'mean_tps'\n",
    "X_train = X_train.drop(target_var, axis=1)\n",
    "X_test = X_test.drop(target_var, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af28291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler()\n",
    "targetStdScaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7763e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = stdScaler.fit_transform(X_train.values)\n",
    "y_train_scaled = targetStdScaler.fit_transform(y_train.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = stdScaler.transform(X_test.values)\n",
    "y_test_scaled = targetStdScaler.transform(y_test.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Nets imports\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b5b4b-5c0a-46c1-8fbe-a4e6c627d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7afdc-69cc-4197-ac03-fd1f03535653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve AWS access from the data connection attached to the workbench\n",
    "aws_access_key_id = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "endpoint_url = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "bucket_name = os.environ[\"AWS_S3_BUCKET\"]\n",
    "region_name = os.environ[\"AWS_DEFAULT_REGION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca337d-6594-43a8-841b-996fb9328d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "verboseLevel=0\n",
    "validationSplit=0.2\n",
    "batchSize=30\n",
    "epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd199ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback preparation\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.5,\n",
    "                              patience=2,\n",
    "                              verbose=verboseLevel,\n",
    "                              mode='min',\n",
    "                              min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = X_train_scaled.shape[1]\n",
    "colList = ['HiddenLayers', 'R2Score', 'MAE', 'MSE', 'MAPE', 'H5FileName', 'TrainHistory', 'TrainPredictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb72f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_loss = 'mae'\n",
    "#loss = 'mse'\n",
    "measure_metrics = ['mae', 'mse']\n",
    "#measure_metrics = ['mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is the reference for creating and training the models inside the Jupyter notebook pod. We will evaluate it against the Ray cluster distributed one\n",
    "def createModel(testResultsFrame, layerSize, loops, target_loss, measure_metrics,\n",
    "                y_train, X_train, y_test, X_test, targetScaler, labelSet):\n",
    "    print(f'Creating model using layer size = {layerSize} on set = {labelSet}.\\n')\n",
    "    for i in range(loops):\n",
    "        print(f'Training on {i} hidden layers\\n')\n",
    "        model = Sequential()\n",
    "        model.add(Dense(layerSize, kernel_initializer='normal',\n",
    "                        input_dim=inputSize, activation='relu'))\n",
    "        for j in range(i):\n",
    "            model.add(Dense(layerSize, \n",
    "                            kernel_initializer='normal', activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(1, kernel_initializer='normal', \n",
    "                        activation='linear'))\n",
    "\n",
    "        optmzr=Adam(learning_rate=0.001)    \n",
    "        model.compile(optimizer=optmzr, loss=target_loss, metrics=measure_metrics)\n",
    "\n",
    "        model_h5_name = 'mlp_' + str(layerSize)+ '_' + str(i) + '_model_std_' + labelSet + '.keras'\n",
    "        checkpoint_nn_std = ModelCheckpoint(model_h5_name,\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=verboseLevel,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='min')\n",
    "        callbacks_list_nn_std = [checkpoint_nn_std, reduce_lr]\n",
    "\n",
    "        history_MLP_std = model.fit(X_train, y_train,\n",
    "                                    batch_size=batchSize, \n",
    "                                    validation_split=validationSplit, \n",
    "                                    epochs=epochs, verbose=verboseLevel,\n",
    "                                    callbacks=callbacks_list_nn_std)\n",
    "\n",
    "        #reload the best model!\n",
    "        model_new = load_model(model_h5_name)\n",
    "        #Predict\n",
    "        y_pred_scaled = model_new.predict(X_test)\n",
    "        #Evaluate metrics\n",
    "        y_pred = targetScaler.inverse_transform(y_pred_scaled)\n",
    "        r2_score = metrics.r2_score(y_test, y_pred)\n",
    "        mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "        mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "        mape = metrics.mean_absolute_percentage_error(y_test, y_pred)\n",
    "        #store values\n",
    "        row = [i, r2_score, mae, mse, mape, model_h5_name, history_MLP_std, y_pred]\n",
    "        df = pd.DataFrame(np.array(row, dtype=object).reshape(1, len(colList)), columns=colList)\n",
    "        testResultsFrame = testResultsFrame.append(df, ignore_index=True)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        del(model)\n",
    "        del(model_new)\n",
    "        \n",
    "    return testResultsFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f914d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "testResDataFrame = pd.DataFrame(columns=colList)\n",
    "layerSize = 64\n",
    "loops = 15\n",
    "testResDataFrame = createModel(testResDataFrame, layerSize, loops, \n",
    "                        target_loss, measure_metrics,\n",
    "                        y_train_scaled, X_train_scaled,\n",
    "                        y_test, X_test_scaled, \n",
    "                        targetStdScaler, 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56145d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.plot(testResDataFrame['R2Score'])\n",
    "plt.plot(testResDataFrame['MAE'])\n",
    "#plt.plot(testResDataFrame['MSE'])\n",
    "plt.title('Training Scores MLP')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(['MAE'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707005cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the IDX value where the MAE is smallest\n",
    "minMaeIDX = testResDataFrame.loc[testResDataFrame['MAE']==testResDataFrame['MAE'].min()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac3e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "testResDataFrame.iloc[minMaeIDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP_std = testResDataFrame['TrainPredictions'][minMaeIDX]\n",
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test.shape[0]),y_test,label=\"Original Data\", alpha=0.6, c='red')\n",
    "plt.scatter(range(y_pred_MLP_std.shape[0]),y_pred_MLP_std,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='black')\n",
    "plt.ylabel('Mean TPS')\n",
    "plt.xlabel('Test Records')\n",
    "plt.title('MLP Std Model for X_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef764d-a830-4793-aca4-b860a9cbf6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_URI='http://mlflow-server.mlflow-server.svc.cluster.local:8080/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77202e7-ccd2-4ad7-86c0-bca5e87f99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"RAY_IGNORE_UNHANDLED_ERRORS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a21de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import ray\n",
    "from codeflare_sdk import TokenAuthentication, Cluster, ClusterConfiguration\n",
    "from codeflare_sdk import generate_cert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d953c-889e-4ce9-9b6a-085dec6bb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = TokenAuthentication(\n",
    "    token = \"sha256~zAPMzZL-O4dWaBL8oMKXk2Wq8UtUiSb4JwrWEiFe3Cs\", # execute ocp whoami -t on the authenticated cluster to obtain the token\n",
    "    server = \"https://api.cluster-ffqgg.ffqgg.sandbox1386.opentlc.com:6443\",\n",
    "    skip_tls = False\n",
    ")\n",
    "auth.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bfa13-5762-4b4f-b02e-a35b6b87b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create required TLS cert and export the environment variables to enable TLS\n",
    "generate_cert.generate_tls_cert('raycluster-complete', 'raycluster')\n",
    "generate_cert.export_env('raycluster-complete', 'raycluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a390d5-0e88-44b0-8fc8-81088070fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_endpoint = 'ray://raycluster-complete-head-svc.raycluster.svc.cluster.local:10001' # ensure your ray cluster URL is correct\n",
    "ray.shutdown()\n",
    "ray.init(address=ray_endpoint, logging_level=logging.ERROR, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb4e80-3604-410b-bf9f-ab369ddc4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8887f-6c0b-45e4-a6c7-a00ffce405cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8217b546-0eb2-44e7-88f2-15d2774aca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.integrations.mlflow import setup_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7815b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def createRemoteModel(layerSize, loops, target_loss, measure_metrics,\n",
    "                y_train, X_train, y_test, X_test, targetScaler, labelSet):\n",
    "        \n",
    "    mlflow_exp_name = f'mlasp-1-{labelSet}-{loops}'\n",
    "    mlflow_ray_config = None\n",
    "    mlflow_ray = setup_mlflow(config=mlflow_ray_config,\n",
    "                              tracking_uri=MLFLOW_URI,\n",
    "                              registry_uri=MLFLOW_URI,\n",
    "                              create_experiment_if_not_exists=True,\n",
    "                              rank_zero_only=False,\n",
    "                             experiment_name=mlflow_exp_name)\n",
    "    \n",
    "    print(f'Creating model using {loops} hidden layers of size = {layerSize} on set = {labelSet}.\\n')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layerSize, kernel_initializer='normal',\n",
    "                    input_dim=inputSize, activation='relu'))\n",
    "    for j in range(loops):\n",
    "        model.add(Dense(layerSize, \n",
    "                        kernel_initializer='normal', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, kernel_initializer='normal', \n",
    "                    activation='linear'))\n",
    "\n",
    "    optmzr=Adam(learning_rate=0.001)    \n",
    "    model.compile(optimizer=optmzr, loss=target_loss, metrics=measure_metrics)\n",
    "\n",
    "    model_h5_name = 'mlp_' + str(layerSize)+ '_' + str(loops) + '_model_std_' + labelSet + '.keras'\n",
    "    checkpoint_nn_std = ModelCheckpoint(model_h5_name,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "    callbacks_list_nn_std = [checkpoint_nn_std, reduce_lr]\n",
    "\n",
    "    history_MLP_std = model.fit(X_train, y_train,\n",
    "                                batch_size=batchSize, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_nn_std)\n",
    "\n",
    "    #reload the best model!\n",
    "    model_new = load_model(model_h5_name)\n",
    "    #Predict\n",
    "    y_pred_scaled = model_new.predict(X_test)\n",
    "    #Evaluate metrics\n",
    "    y_pred = targetScaler.inverse_transform(y_pred_scaled)\n",
    "    r2_score = metrics.r2_score(y_test, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    mape = metrics.mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    fig=plt.figure(figsize=(20,10))\n",
    "    plt.scatter(range(y_test.shape[0]),y_test,label=\"Original Data\", alpha=0.6, c='red')\n",
    "    plt.scatter(range(y_pred.shape[0]),y_pred,label=\"Predicted Data\", \n",
    "                alpha=0.6, c='black')\n",
    "    plt.ylabel('Mean TPS')\n",
    "    plt.xlabel('Test Records')\n",
    "    plt.title('MLP StdScaler Model for X_test dataset prediction vs original')\n",
    "    plt.legend()    \n",
    "\n",
    "    mlflow_ray.log_figure(fig,f\"{mlflow_exp_name}.png\")\n",
    "    \n",
    "    mlflow_ray.log_param(\"batch_size\", batchSize)\n",
    "    mlflow_ray.log_param(\"layer_size\", layerSize)\n",
    "    mlflow_ray.log_param(\"hidden_layers\", loops)\n",
    "    mlflow_ray.log_param(\"activation_function\", \"relu\")\n",
    "    mlflow_ray.log_param(\"dense_kernel_initializer\", \"normal\")\n",
    "    mlflow_ray.log_param(\"epochs\", epochs)\n",
    "    mlflow_ray.log_param(\"learning_rate\", 0.001)\n",
    "    mlflow_ray.log_param(\"optimizer\", \"adam\")\n",
    "\n",
    "    mlflow_ray.log_metric(\"mae\", mae)\n",
    "    mlflow_ray.log_metric(\"mse\", mse)\n",
    "    mlflow_ray.log_metric(\"mape\", mape)\n",
    "    mlflow_ray.log_metric(\"r2_score\", r2_score)\n",
    "    \n",
    "    model_onnx,_ = tf2onnx.convert.from_keras(model_new)\n",
    "    mlflow_ray.onnx.log_model(model_onnx, f\"model-{mlflow_exp_name}\")\n",
    "\n",
    "    row = [loops, r2_score, mae, mse, mape]\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376c98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "testResDataFrame2 = []\n",
    "layerSize = 64\n",
    "loops = 15\n",
    "\n",
    "\n",
    "for i in range(loops):\n",
    "    rowResult = createRemoteModel.remote(layerSize, i, \n",
    "                        target_loss, measure_metrics,\n",
    "                        y_train_scaled, X_train_scaled,\n",
    "                        y_test, X_test_scaled, \n",
    "                        targetStdScaler, labelSet='all_ray')\n",
    "    testResDataFrame2.append(rowResult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9db43-5d70-4421-b06c-3be7c0e5d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "testResDataFrame2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47a97e-eda0-482c-b0d3-7a4b4c3991c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tResDF2 = ray.get(testResDataFrame2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348862fa-65a6-4dd2-810c-9d3f6256a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "colList2=colList.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f183e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tResDF2, columns=colList2[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9748269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaeIDX_ray = df.loc[df['MAE']==df['MAE'].min()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078966bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[minMaeIDX_ray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07e0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.plot(df['R2Score'])\n",
    "plt.plot(df['MAE'])\n",
    "#plt.plot(df['MSE'])\n",
    "plt.title('Training Scores MLP')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(['MAE'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e7a27-4151-4b8e-bcba-f288f632d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b24e8-a8e9-4a95-b7e8-46ba09ad38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = [[True, 21, 277, 1712, 262, 7, 31, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1546087",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rec = stdScaler.transform(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf21aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50cf02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = load_model(testResDataFrame['H5FileName'][minMaeIDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e6273-16be-4a4e-ba84-da3454406c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_model.predict(test_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff876c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ccfdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetStdScaler.inverse_transform(test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285a26a-d409-4565-a0c4-f78088b1bede",
   "metadata": {},
   "source": [
    "### Save the scalers for the inference calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50071874",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(targetStdScaler,'target_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(stdScaler,'standard_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f194a30d-e138-4560-b138-be1350e48f72",
   "metadata": {},
   "source": [
    "### Export to ONNX to run on the RHODS model server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02097f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onnx, _ = tf2onnx.convert.from_keras(test_model, output_path='tf_mlasp.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8893b44-8218-4c4a-83ee-7953c889344b",
   "metadata": {},
   "source": [
    "### Upload model to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacaca1-4b2e-4ed9-a4d1-156cb31477a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a16a30-4dc3-44e6-97b3-b5223e1a1825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#S3 ODF GW client\n",
    "s3_odf = boto3.client(service_name = 's3',\n",
    "                      aws_access_key_id = aws_access_key_id,\n",
    "                      aws_secret_access_key = aws_secret_access_key,\n",
    "                      region_name = 'default',\n",
    "                      endpoint_url = endpoint_url,\n",
    "                      config = botocore.client.Config(signature_version = 's3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f506d-4b61-4fbd-96b7-1ddc766aefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_odf.upload_file('tf_mlasp.onnx', bucket_name, 'models/tf_mlasp.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88393ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_odf.upload_file('lt_results_2022-10-01.csv', bucket_name, 'data/lt_results_2022-10-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b4ff2-5eac-40f6-acc2-135b42ab52b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
